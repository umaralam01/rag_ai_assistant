{
  "pages": [
    {
      "page": 1,
      "text": "1747037056197-webscraping.md\n2025-05-16\n1 / 12\nData Collection Techniques\nIn the world of data science, data collection is the cornerstone of all analytical efforts. As the saying goes,\n\"Garbage In, Garbage Out\"—the quality of insights directly depends on the quality of data collected. Whether\nyou're building machine learning models, performing statistical analysis, or developing AI systems, your\nsuccess begins with gathering accurate, reliable data.\nUnderstanding Data Sources\nData can come from a variety of sources:\nAPIs provide structured access to real-time information, such as weather or stock data.\nDatabases (both SQL and NoSQL) house large volumes of structured data.\nWebpages offer valuable unstructured data from blogs, e-commerce platforms, and more.\nFiles like CSV, JSON, and Excel sheets serve as local or cloud-based data repositories.\nSensors and logs provide real-time inputs from IoT devices and application events.\nEthical Considerations\nEthical data collection is crucial. Always review a website’s Terms of Service and respect robots.txt when\nscraping. Data must be collected with informed consent and in compliance with regulations such as GDPR and\nCCPA. Attribution, licensing, and data usage rights should never be overlooked.\nTools and Techniques\nA variety of tools support data collection:\nAPIs (REST or GraphQL) for structured, remote data access.\nWeb Scraping using tools like BeautifulSoup, Scrapy, or Selenium for HTML parsing.\nWebhooks for event-based data (e.g., payment notifications).\nManual methods like forms or surveys for crowd-sourced insights.\nFile handling with Python libraries such as pandas, csv, and json.\nFor working with databases, Python libraries like sqlite3, SQLAlchemy, and pymongo are widely used to\ninteract with both relational and NoSQL systems.\nFinal Takeaway\nData collection is more than just gathering information—it’s about doing so responsibly, efficiently, and with\nthe right tools. Python stands out as a versatile language, offering powerful libraries for virtually every type of\ndata source. Mastering these techniques lays a strong foundation for any data science project.\nWhat is Web Scraping?\nWeb Scraping is the automated process of extracting information from websites.\nInstead of manually copying data, a scraper reads and parses the webpage's code to collect the\nneeded data.\nUsed widely in data science for collecting:"
    },
    {
      "page": 2,
      "text": "1747037056197-webscraping.md\n2025-05-16\n2 / 12\nProduct prices\nNews articles\nJob listings\nResearch datasets\nWhy Use Web Scraping?\nAutomate repetitive data collection tasks\nAccess data that is not available via APIs\nBuild datasets for Machine Learning models and Analytics\nMonitor websites for price changes, content updates, or news alerts\nWhen Not to Use Web Scraping\nIf the site offers an official API, prefer that (it's cleaner and more stable).\nIf scraping violates the site's Terms of Service.\nIf the scraping harms the website (excessive requests can cause server overload).\nHTML Basics for Web Scraping\nWhat is HTML?\nHTML (HyperText Markup Language) structures content on the web.\nIt's made up of elements (tags) like <div>, <p>, <h1>, <a>, etc.\nExample:\n<html> \n  <body> \n    <h1>Product Title</h1> \n    <p class=\"price\">$29.99</p> \n    <a href=\"/buy-now\">Buy Now</a> \n  </body>\n</html>\nImportant HTML Elements for Scraping\nTag\nMeaning\nCommon Use\n<div>\nDivision/Container\nGroup content\n<p>\nParagraph\nText blocks\n<h1>, <h2>, etc.\nHeadings\nTitles and sections\n<a>\nAnchor (links)\nURLs, navigation"
    },
    {
      "page": 3,
      "text": "1747037056197-webscraping.md\n2025-05-16\n3 / 12\nTag\nMeaning\nCommon Use\n<img>\nImage\nPictures and icons\n<table>\nTable\nStructured tabular data\nAttributes Matter!\nHTML tags often have attributes like id, class, href, src.\nWe use these attributes to target the correct elements.\nExample:\n<p class=\"price\">$29.99</p>\nHere, the class=\"price\" attribute helps identify the price on the page.\nQuick CSS Basics for Scraping\nWhat is CSS?\nCSS (Cascading Style Sheets) controls the style and layout of HTML elements.\nFor scraping, we mainly care about CSS selectors to find and extract data.\nCommon CSS Selectors\nSelector\nMeaning\nExample\n.class\nSelects elements by class\n.price, .title\n#id\nSelects an element by id\n#main, #product-title\ntag\nSelects all elements of a tag\nh1, div, p\ntag.class\nSelects a tag with class\np.price, div.container\nExample: Selecting Elements\nHTML:\n<p class=\"price\">$29.99</p>\nCSS selector to target this:\np.price"
    },
    {
      "page": 4,
      "text": "1747037056197-webscraping.md\n2025-05-16\n4 / 12\nIn Python using BeautifulSoup:\nsoup.select_one(\"p.price\") \nTools We'll Use for Web Scraping\nrequests → To download the HTML content of a webpage.\nBeautifulSoup → To parse and extract data from the HTML.\n(Optional) Selenium → For websites that load data dynamically with JavaScript.\nSummary\nWeb scraping automates data extraction from websites.\nUnderstanding basic HTML structure and CSS selectors is crucial.\nPython provides powerful libraries to make web scraping easy.\nWhat is HTML\nHTML (HyperText Markup Language) is the standard language used to structure content on the web. When\nyou load a web page, your browser interprets HTML to display the layout, text, images, and other content.\nBasic Structure of an HTML Document\n<!DOCTYPE html>\n<html> \n  <head> \n    <title>Sample Page</title> \n  </head> \n  <body> \n    <h1>Welcome!</h1> \n    <p>This is a sample page.</p> \n  </body>\n</html>\nKey Elements\n<!DOCTYPE html>: Declares the document type.\n<html>: Root element of the page.\n<head>: Contains metadata, styles, and scripts.\n<body>: Contains visible content.\nCommon HTML Tags Used in Scraping"
    },
    {
      "page": 5,
      "text": "1747037056197-webscraping.md\n2025-05-16\n5 / 12\nMost Frequently Encountered Tags\nTag\nPurpose\n<div>\nSection or container for content\n<span>\nInline container\n<a>\nAnchor tag for hyperlinks\n<img>\nDisplays images (uses src)\n<ul>, <ol>, <li>\nLists and list items\n<table>, <tr>, <td>\nTables and cells\n<h1> to <h6>\nHeaders (various sizes)\n<p>\nParagraph text\n<form>, <input>, <button>\nForm elements\nAttributes in HTML\nHTML tags often include attributes that provide metadata or instructions:\n<a href=\"https://example.com\" class=\"nav-link\">Visit Site</a>\nCommon Attributes\nhref: Hyperlink reference\nsrc: Image or media source\nclass: CSS class (commonly used for scraping)\nid: Unique identifier for an element\nname: Often used in form elements\ntype: Used in <input> tags\nNavigating HTML Structure in Scraping\nScraping tools like BeautifulSoup and Selenium use tag names and attributes to locate elements.\nExample Targets\nBy tag: soup.find('div')\nBy class: soup.find('div', class_='product')\nBy ID: soup.find(id='header')\nBy attribute: soup.find('a', {'href': True})\nUnderstanding Nested Elements"
    },
    {
      "page": 6,
      "text": "1747037056197-webscraping.md\n2025-05-16\n6 / 12\nHTML is hierarchical. Tags can contain other tags.\n<div class=\"article\"> \n  <h2>Title</h2> \n  <p>This is a summary.</p>\n</div>\nTo extract both the title and summary, first locate the parent <div class=\"article\">, then its children.\nPractical Tips\nUse browser DevTools (Right-click > Inspect) to examine HTML structure.\nTarget elements with unique id or descriptive class attributes.\nUse tag nesting logic to extract specific parts of a page.\nUseful Python Libraries for HTML Parsing\nrequests: For sending HTTP requests\nBeautifulSoup: For parsing and traversing HTML\nlxml: Fast parser for large documents\nSelenium: For interacting with JavaScript-rendered pages\nUsing Requests for Web Scraping\nToday we will see how to scrape websites and use requests module to download the raw html of a webpage.\nIn this section we can safely use https://quotes.toscrape.com/ and https://books.toscrape.com/ for scraping\ndemos\n1. What is requests?\nrequests is a Python library used to send HTTP requests easily.\nIt allows you to fetch the content of a webpage programmatically.\nIt is commonly used as the first step before parsing HTML with BeautifulSoup.\n2. Installing requests\nTo install requests, run:\npip install requests \n3. Sending a Basic GET Request"
    },
    {
      "page": 7,
      "text": "1747037056197-webscraping.md\n2025-05-16\n7 / 12\nExample\nimport requests \n \nurl = \"https://example.com\" \nresponse = requests.get(url) \n \n# Print the HTML content \nprint(response.text) \nKey points:\nurl: The website you want to fetch.\nresponse.text: The HTML content of the page as a string.\n4. Checking the Response Status\nAlways check if the request was successful:\nprint(response.status_code) \nCommon Status Codes\n200: OK (Success)\n404: Not Found\n403: Forbidden\n500: Internal Server Error\nGood practice:\nif response.status_code == 200: \n    print(\"Page fetched successfully!\") \nelse: \n    print(\"Failed to fetch the page.\") \n5. Important Response Properties\nProperty\nDescription\nresponse.text\nHTML content as Unicode text\nresponse.content\nRaw bytes of the response\nresponse.status_code\nHTTP status code"
    },
    {
      "page": 8,
      "text": "1747037056197-webscraping.md\n2025-05-16\n8 / 12\nProperty\nDescription\nresponse.headers\nMetadata like content-type, server info\n6. Adding Headers to Mimic a Browser\nSometimes websites block automated requests. Adding a User-Agent header helps the request look like it is\ncoming from a real browser.\nheaders = { \n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\" \n} \n \nresponse = requests.get(url, headers=headers) \n7. Handling Connection Errors\nWrap your request in a try-except block to handle errors gracefully:\ntry: \n    response = requests.get(url, timeout=5) \n    response.raise_for_status()  # Raises an HTTPError for bad responses \n    print(response.text) \nexcept requests.exceptions.RequestException as e: \n    print(f\"An error occurred: {e}\") \n8. Best Practices for Fetching Pages\nAlways check the HTTP status code.\nUse proper headers to mimic a browser.\nSet a timeout to avoid hanging indefinitely.\nRespect the website by not making too many rapid requests.\n9. Summary\nrequests makes it simple to fetch web pages using Python.\nIt is the starting point for most web scraping workflows.\nCombining requests with BeautifulSoup allows for powerful data extraction.\nUsing BeautifulSoup for Web Scraping\n1. What is BeautifulSoup?"
    },
    {
      "page": 9,
      "text": "1747037056197-webscraping.md\n2025-05-16\n9 / 12\nBeautifulSoup is a Python library used to parse HTML and XML documents.\nIt creates a parse tree from page content, making it easy to extract data.\nIt is often used with requests to scrape websites.\n2. Installing BeautifulSoup\nInstall both beautifulsoup4 and a parser like lxml:\npip install beautifulsoup4 lxml \n3. Creating a BeautifulSoup Object\nExample\nfrom bs4 import BeautifulSoup \nimport requests \n \nurl = \"https://example.com\" \nresponse = requests.get(url) \n \nsoup = BeautifulSoup(response.text, \"lxml\") \nresponse.text: HTML content.\n\"lxml\": A fast and powerful parser (you can also use \"html.parser\").\n4. Understanding the HTML Structure\nBeautifulSoup treats the page like a tree.\nYou can search and navigate through tags, classes, ids, and attributes.\nExample HTML:\n<html> \n  <body> \n    <h1>Title</h1> \n    <p class=\"description\">This is a paragraph.</p> \n    <a href=\"/page\">Read more</a> \n  </body>\n</html>\n5. Common Methods in BeautifulSoup"
    },
    {
      "page": 10,
      "text": "1747037056197-webscraping.md\n2025-05-16\n10 / 12\n5.1 Accessing Elements\nAccess the first occurrence of a tag:\nsoup.h1 \nGet the text inside a tag:\nsoup.h1.text \n5.2 find() Method\nFinds the first matching element:\nsoup.find(\"p\") \nFind a tag with specific attributes:\nsoup.find(\"p\", class_=\"description\") \n5.3 find_all() Method\nFinds all matching elements:\nsoup.find_all(\"a\") \n5.4 Using select() and select_one()\nSelect elements using CSS selectors.\nsoup.select_one(\"p.description\") \nsoup.select(\"a\") \n6. Extracting Attributes\nGet the value of an attribute, such as href from an <a> tag:"
    },
    {
      "page": 11,
      "text": "1747037056197-webscraping.md\n2025-05-16\n11 / 12\nlink = soup.find(\"a\") \nprint(link[\"href\"]) \nOr using .get():\nprint(link.get(\"href\")) \n7. Traversing the Tree\nAccess parent elements:\nsoup.p.parent \nAccess children elements:\nlist(soup.body.children) \nFind the next sibling:\nsoup.h1.find_next_sibling() \n8. Handling Missing Elements Safely\nAlways check if an element exists before accessing it:\ntitle_tag = soup.find(\"h1\") \nif title_tag: \n    print(title_tag.text) \nelse: \n    print(\"Title not found\") \n9. Summary\nBeautifulSoup helps parse and navigate HTML easily.\nUse .find(), .find_all(), .select(), and .select_one() to locate data.\nAlways inspect the website's structure before writing scraping logic."
    },
    {
      "page": 12,
      "text": "1747037056197-webscraping.md\n2025-05-16\n12 / 12\nCombine BeautifulSoup with requests for full scraping workflows."
    }
  ]
}